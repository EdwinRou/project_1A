# -*- coding: utf-8 -*-
"""projet_info.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17nRhiQiTHkctKcOkCeJRzdqJJuLyYucl

# Projet d'informatique : Réseaux de neuronnes.
Par Edwin Roussin et Alexandre Partensky.

## Gestion de nos données

 La base de données MNIST est un ensemble d'image de chiffre de résolution 28x28, chacune accompagné de leur valeur représentée sous forme d'un entier ou d'un vecteur. Après avoir téléchargé la base de données, nous l'ouvrons à l'aide d'un code que nous avons repris sur Github.

> Lien base MNIST : http://yann.lecun.com/exdb/mnist/



> Lien github : https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/mnist_loader.py



  Les images sont triées selon deux groupes. Le premier, appelé "training_data" permet d'entrainer l'algorithme de machine learning à reconnaître les images. Le second, "test_data" permet de tester la capacité de l'algorithme à reconnaître des images, une fois que celui-ci à été entrainé sur les images d'entraînements. Ces deux objets sont importés sous forme de liste de couple, chaque couple représentant une image avec l'ensemble des valeurs des pixels pour la première composante et la traduction de la valeur de l'image pour la seconde composante. Pour le test_data, la valeur de chaque image (deuxième composante du tuple) est un nombre tandis que c'est une liste sous la forme $np.array([0,1,0,0,0,0,0,0,0,0])$ pour le trainig_data. La fonction "vectorized_result" permet de passer de la première représentation à la deuxième si besoin.
  
Il existe également un validation_data utilisé de manière intermédiaire dans la phase d'entrainement qui comporte moins de données que le test_data. Il est surtout utile pour calibrer les hyperparamètres du réseau, prévenir l'over-fitting et mettre en place des early-stopping par exemple.
"""

#importation des données MNIST
import pickle
import gzip

import numpy as np

def load_data():
    """Return the MNIST data as a tuple containing the training data,
    the validation data, and the test data.
    The ``training_data`` is returned as a tuple with two entries.
    The first entry contains the actual training images.  This is a
    numpy ndarray with 50,000 entries.  Each entry is, in turn, a
    numpy ndarray with 784 values, representing the 28 * 28 = 784
    pixels in a single MNIST image.
    The second entry in the ``training_data`` tuple is a numpy ndarray
    containing 50,000 entries.  Those entries are just the digit
    values (0...9) for the corresponding images contained in the first
    entry of the tuple.
    The ``validation_data`` and ``test_data`` are similar, except
    each contains only 10,000 images.
    This is a nice data format, but for use in neural networks it's
    helpful to modify the format of the ``training_data`` a little.
    That's done in the wrapper function ``load_data_wrapper()``, see
    below.
    """
    f = gzip.open('mnist.pkl.gz', 'rb')
    training_data, validation_data, test_data = pickle.load(f, encoding="latin1")
    f.close()
    return (training_data, validation_data, test_data)

def load_data_wrapper():
    """Return a tuple containing ``(training_data, validation_data,
    test_data)``. Based on ``load_data``, but the format is more
    convenient for use in our implementation of neural networks.
    In particular, ``training_data`` is a list containing 50,000
    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray
    containing the input image.  ``y`` is a 10-dimensional
    numpy.ndarray representing the unit vector corresponding to the
    correct digit for ``x``.
    ``validation_data`` and ``test_data`` are lists containing 10,000
    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional
    numpy.ndarry containing the input image, and ``y`` is the
    corresponding classification, i.e., the digit values (integers)
    corresponding to ``x``.
    Obviously, this means we're using slightly different formats for
    the training data and the validation / test data.  These formats
    turn out to be the most convenient for use in our neural network
    code."""
    tr_d, va_d, te_d = load_data()
    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]
    training_results = [vectorized_result(y) for y in tr_d[1]]
    training_data = zip(training_inputs, training_results)
    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]
    validation_data = zip(validation_inputs, va_d[1])
    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]
    test_data = zip(test_inputs, te_d[1])
    return (training_data, validation_data, test_data)

def vectorized_result(j):
    """Return a 10-dimensional unit vector with a 1.0 in the jth
    position and zeroes elsewhere.  This is used to convert a digit
    (0...9) into a corresponding desired output from the neural
    network."""
    e = np.zeros((10, 1))
    e[j] = 1.0
    return e

training_data, validation_data, test_data = load_data_wrapper()

"""On importe d'abord les librairies nécessaires à la construction de notre réseau."""

import numpy as np
import random
from tqdm import tqdm #sert à visualiser l'avancement des calculs lorsqu'on lance le programme
import matplotlib.pyplot as plt

"""On définit deux fonctions, $sigmoid$ et $sigmoid$_$prime$. La fonction $sigmoid$ est la fonction d'activation de notre réseau. Elle est valeur de sortie d'un neuronne à partir d'une valeur d'entrée $z$ calculé selon les poids et les biais propre au neuronne. Elle est bijective sur $R$ à valeur dans $[0;1]$. 

La fonction $sigmoid$_$prime$ renvoie sa dérivée, ce qui est utile pour la descente de gradient.

"""

def sigmoid(z):
  return 1/(1+np.exp(-z)) # ATTENTION peut valoir 0 ou 1 dans python 

def sigmoid_prime(z): #dérivé de la fonction sigmoid
  return sigmoid(z)*(1-sigmoid(z))

"""On construit alors le réseau à l'aide de la classe "Network".

## Description du type de neuronne utilisé

Le neuronne utilisé est classique.

Pour une liste d'entrée  $ (e_{1},e_{2},e_{3},...,e_{n})$, Il définit en entier $z$ tel que 

$z = w_{1}×e_{1} + w_{2}×e_{2} + ... + w_{n}×e_{n} + b $

avec $(w_{1},w_{2},w_{3},...,w_{n})$ l'ensemble des "poids" associé à un neuronne et $b$ le biais du neuronne. 

On compose ensuite z par la fonction d'activation sigmoid définie ci-dessus. Le neuronne renvoie alors sigmoid(z). 

Pour une couche de neuronnes, prenant en entrée $E = (e_{1},e_{2},e_{3},...,e_{n})$, on peut calculer la valeur de sortie sous forme d'un vecteur $A$ tel que 

$A = sigmoid(Z) = W·E + B $

avec $A$ est le vecteur d'activation de la couche en question, $W$ la matrice des poids des neuronnes de la couche, dont chaque ligne est l'ensemble des poids d'un seul neuronne et $B$ le vecteur des biais des neuronnes de la couche

$A = sigmoid(Z)$ est le vecteur de sortie de la couche de neuronne dont les composantes sont $(sigmoid(z_{1}),sigmoid(z_{2}),...,sigmoid(z_{n})$.

### Description des attributs :

La classe contient 6 attributs :

- **num_layers** est le nombre de couche de neuronnes de notre réseau
>**Par soucis de simplicité, nous décidons d'utiliser uniquement un réseau de neuronnes avec 3 couches, une pour les entrées, une au milieu et une pour les sorties**.
- **sizes** est un tableau de taille num_layers et dont la i-ème case est le nombre de neuronnes dans la i-ème couche du réseau.

- **biaises** est la liste des vecteurs des biais de chaque couche de neuronnes.

- **weights** est la liste des matrices des poids de chaque couche de neuronnes.

- **train_cost** est la liste des costs sur le train set lors de l'entraînements.

- **test_cost** est la liste des costs sur le test set lors de l'entraînement
- **acc** la liste donnant l'accuracy du réseau sur le test set au fur et à mesure de l'entraînement


### Description des premières méthodes :

- **init** correspond à l'initialisation de la classe, elle prend en argument la taille de réseaux, soit une liste de longueur variable dont la valeur de la i-ième case est le nombre de neuronne de la i-ème couche.

 Elle prend également en argument, si spécifié, une classe Cost. Cela permet de tester différentes fonctions coût. Nous avons par exemple rajouté la fonction log_loss à notre réseaux. Cela permet d'accéler l'apprentissage surtout au début si le score est très mauvais.

>La première couche, comporte $28*28 = 784$ entrées, soit une par pixel.

>La dernière couche, comporte 10 sorties, la valeur de la i-ème sortie est la probabilité que le chiffre soit égal à i, elle est donc comprise entre 0 et 1 (la fonction sigmoïd est bien à valeur dans $[0;1]$). 
Pour intépreter ce résultat, on considère que le chiffre choisis par l'algorithme est celui dont la probabilité est la plus élévée. 

>Comme nous décidons d'utiliser un réseau avec seulement 3 couches, on n'utilise $init$ uniquement avec un tableau de taille 3.


- **vectorized_result** transforme un chiffre en une représentation sous forme de vecteur qui est semblable à celle de sortie du réseau. Le chiffre i est renvoyé sous la forme d'un vecteur de taille 10 dont toutes les composantes sont nulles sauf la i-ème qui vaut 1. 

- **feedforward** prend en argument la liste des pixels d'une image et renvoie à l'aide d'une suite de calculs matriciels le vecteur de sortie de notre réseau. 

- **accuracy** prend en argument la liste d'image test_data et renvoie le taux de succes de l'algorithme de reconnaissance ainsi que le cost total.



- **progression**  affiche avec des graphes l'erreur cumulée et la précision du réseau de neuronnes.

>On définit une nouvelle classe à chaque fois qu'on souhaite utiliser une nouvelle fonciton coût. Ici il y a le classique QuadraticCost ainsi que la CrossEntropyCost. Pour chaque coût on implémente la fonction coût en elle même ainsi que la fonction delta qui dépend mathématiquement de la dérivée de la fonction coût. 

>La différence majeure entre les deux est que le delta pour la fonction log loss ne dépend plus de $sigma$_$prime(z)$ qui peut être très faible alors que le reseaux se trompent complétement. Ainsi on est assuré que delta ne sera jamais trop petit lorsque l'output du reseau est très différent de la réponse attendu, ce qui n'était pas le cas avec le QuadraticCost.
"""

class QuadraticCost(object):
    @staticmethod
    def cost(a, y):
      if type(y) == np.int64 :
        y = vectorized_result(y)
      return 0.5*np.linalg.norm(a-y)**2
    @staticmethod
    def delta(z, a, y):
        return (a-y) * sigmoid_prime(z)


class CrossEntropyCost(object):

    @staticmethod
    def cost(a, y):
      if type(y) == np.int64 :
        y = vectorized_result(y)
      return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))

    @staticmethod
    def delta(z, a, y):
        return (a-y)

"""### Description des méthodes relatives à l'apprentissage du réseau de neuronnes : 

- **SGD** est la méthode qui excécute l'apprentissage de notre réseau de neurones.

Elle prend donc "training_data" en premier argument. 
"nb_training" est le nombre de fois que le programme répétera l'apprentissage sur l'ensemble du "training_data".

Lors de l'apprentisssage celui-ci est divisé un nombre de lots (batch) de taille batch_size (troisième argument). Le reseau de neurone apprendra ainsi batch après batch.

"eta" est le learning rate, un des hyperparamètres de notre réseau, qui comme son nom l'indique peut de manière analogique est  vue comme une vitesse d'apprentissage du  réseau.

Enfin "test_data" permet entre chaque entraînement sur le "training_data" d'observer l'évolution des performances de l'algorithme sur des données en dehors du set d'apprentissage.

En particulier, SGD divise le l'ensemble des images de training_data en différents lots. Lorsque l'algorithme finit de parcourir un lot, SGD appelle la méthode **update_batch** qui va modifier les weights et les biaises du réseau selon la méthode de descente de gradient obtenue par la méthode **backprop**. 

Séparer en différents lots permet d'éviter que l'algorithme apprenne à chaque image et donc que le temps d'exécution du programme ne soit trop long.

- **update_batch** est une méthode appelée par SGD, qui permet l'apprentissage du réseau lorsqu'on lui fait parcourir un lot d'image. Elle prend donc en argument un batch, et eta, la vitesse d'apprentissage. Pour chaque élément du batch, elle récolte dw et db, soit les modifications du réseau nécéssaire, obtenue par la méthode de descente de gradient et calculé ici par la méthode **backprop**. Elle en fait une moyenne et modifie adéquatement les valeurs de weights et biaises. Plus eta est élevé, plus la modification sera importante.

- **backprop** est une méthode qui prend en argument une image, donc ici la liste de valeur de ses pixels et le nombre qu'elle représente.*La méthode calcule d'abord l'output du réseau pour cette image qui est un vecteur numpy de taille 10 avec des nombre dans $[0,1]$.

 Elle calcule ensuite, selon la méthode de descente du gradient, dw et db, qui sont les modifications qu'il faut apporter au réseaux de neuronnes afin de faire baisser le coût de manière optimale.
 
 La méthode utilisé s'appelle backpropagation. L'idéee générale est de d'abord calculer les dérivées partielles de C (fonction coût quelconque) par rapport au biais et poids de la dernière couche, puis grâce à ces valeurs de calculer les valeurs de la couche précendete etc jusqu'à tous les connaîtres.


Les équations utilisées pour la backpropagation sont les suivantes :

![Capture d’écran 2022-05-05 184257.png](data:image/png;base64,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)

"""

import numpy as np
import random
from tqdm import tqdm
import matplotlib.pyplot as plt


def sigmoid(z):
  return 1/(1+np.exp(-z)) # ATTENTION peut valoir 0 ou 1 dans python 

def sigmoid_prime(z): #dérivé de la fonction sigmoid
  return sigmoid(z)*(1-sigmoid(z))

class Network(object):

    def __init__(self, sizes,  Cost = CrossEntropyCost): #sizes = [784,_,10] = [taille_data, taille_layer_1, output_layer]
        self.num_layers = len(sizes)-1
        self.sizes = sizes
        self.cost = Cost
        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
        self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(sizes[:-1], sizes[1:])]
        self.train_cost = []
        self.test_cost = []
        self.acc = []
        #initialize biases and weights randomly

    
    def feedforward(self, value):
        for i in range(self.num_layers):
           value = sigmoid(np.dot(self.weights[i],value) + self.biases[i])
        return value

    # Test unitaire : 

    def test_feedforward(self): #vérifie que feedforward() renvoie bien le type de valeur souhaité
      data = np.ones((784,1))
      result = self.feedforward(data)
      return np.shape(result) == (10,1) and np.max(result) <= 1 and np.min(result)>=0


    def accuracy(self, test_data):
      total = 10000 
      sucess = 0
      cost = 0
      for (value,number) in test_data :
        a = self.feedforward(value)
        if np.argmax(a) == number :
          sucess +=1
        cost += (self.cost).cost(a,number)
      return sucess/total, cost/total
 
    
    def SGD(self,training_data, nb_training, batch_size, eta, test_data = None):
      training_data = list(training_data)
      n = len(training_data) 
      self.test_cost.append(batch_size) # store value of batch_size for progression()
      self.test_cost.append(n) # store value of len(training_data) for progression()
      for j in tqdm(range(nb_training)):
          random.shuffle(training_data)
          if n%batch_size == 0 :
              batches = [training_data[i:i + batch_size] for i in range(0,n-batch_size+1, batch_size)]
          else :
              batches = [training_data[i:i + batch_size] for i in range(0,n-batch_size+1, batch_size)] + [training_data[n-n%batch_size:]]
          for batch in (batches):
              self.update_batch(batch, eta)
          if test_data :
              test_data = list(test_data)
              acc , mean_cost = self.accuracy(test_data)
              self.test_cost.append(mean_cost)
              self.acc.append(acc)
              print("Accuracy de {}% après {} entraînement(s)".format(100*acc, j+1))
              
    def update_batch(self, batch, eta):
        
        m = len(batch)
        sum_dw = [np.zeros((y, x)) for x, y in zip(self.sizes[:-1], self.sizes[1:])]
        sum_db = [np.zeros((y, 1)) for y in self.sizes[1:]]
        batch_cost = 0
        for value,number in batch :
            list_dw, list_db, cost = self.backprop(value, number)
            sum_dw = [w + dw for w,dw in zip(sum_dw,list_dw)] 
            sum_db = [b + db for b,db in zip(sum_db,list_db)] 
            batch_cost += cost
        
        self.weights = [w-(eta/m)*dw for w,dw in zip(self.weights,sum_dw)]
        self.biases = [b-(eta/m)*db for b,db in zip(self.biases,sum_db)]
        self.train_cost.append(batch_cost/m)

    def backprop(self, value, number):
        
      z = [np.zeros((y, 1)) for y in self.sizes[1:]]
      db = [np.zeros((y, 1)) for y in self.sizes[1:]]
      dw = [np.zeros((y, x)) for x, y in zip(self.sizes[:-1], self.sizes[1:])]            
      output = value
      a = [value]
      
      for i in range(self.num_layers):
        z[i] = np.dot(self.weights[i],output) + self.biases[i]
        output = sigmoid(z[i])
        a.append(output)
      cost = (self.cost).cost(output, number)
      delta = (self.cost).delta(z[-1], output, number)
      db[-1] = delta
      dw[-1] = np.outer(delta,a[-2])
      
      for j in range(2,self.num_layers+1):
        delta = np.dot(np.transpose(self.weights[-j+1]),delta) * sigmoid_prime(z[-j])
        db[-j] = delta
        dw[-j] = np.outer(delta,a[-j-1])
      return (dw,db,cost)
  
    def progression(self):
        batch_size, nb_data = self.test_cost.pop(0), self.test_cost.pop(0)
        train_cost_by_training = self.train_cost[::nb_data//batch_size]
        plt.subplot(1, 2, 1)
        plt.xlabel("Nb of trainings")
        plt.plot(self.test_cost, label = 'test_cost')
        plt.plot(train_cost_by_training, label = 'train_cost')
        plt.legend()
        plt.title("Costs over trainings")
        plt.subplot(1, 2, 2)
        plt.xlabel("Nb of trainings")
        plt.plot(self.acc)
        plt.title("Accuracy")
        plt.suptitle("Performance")
        plt.show()

"""La cellule suivante affiche les graphes des costs et de la précision obtenus à chaque apprentissage du réseau de neuronnes. On voit bien que l'erreur tend à décroître et que la précision est strictement croissante comme attendu. L'accuracy obtenu après 10 phases d'entraînements est de 95% et ne progresse que très lentement à partir de cette valeur. Entraîner plus le réseau nous expose alors à des risques d'overfitting."""

training_data, validation_data, test_data = load_data_wrapper()

net_0 = Network([784,30,10], QuadraticCost)
net_0.test_feedforward()

net_0.SGD(training_data, 10, 10, 0.05, test_data) 
net_0.progression()

training_data, validation_data, test_data = load_data_wrapper()

net = Network([784,30,10])
net.test_feedforward()


net.SGD(training_data, 100, 10, 0.01, test_data)
net.progression()